{"paragraphs":[{"text":"%dep\n//Adding required packages dynamically. This must be executed first\nz.addRepo(\"Spark Packages Repo\").url(\"http://dl.bintray.com/spark-packages/maven\")\n//z.addRepo(\"OSS SNAPSHOTS\").url(\"https://oss.sonatype.org/content/repositories/snapshots\") //multiple repos can be added.\nz.load(\"FRosner:drunken-data-quality:4.1.1-s_2.11\")\nz.load(\"com.databricks:spark-xml_2.11:0.6.0\")\n// z.load(\"com.databricks:spark-avro_2.11:3.2.0\") //If you re using older version of spark. As of 2.4.2 avro is native.\nz.load(\"org.apache.spark:spark-avro_2.11:2.4.4\")\nz.load(\"io.delta:delta-core_2.11:0.4.0\")\n\n//Get started on databricks using this link: https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-extract-load-sql-data-warehouse","user":"admin","dateUpdated":"2019-10-21T01:10:40+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@2656e85b\n"}]},"apps":[],"jobName":"paragraph_1571581866500_-1497291322","id":"20191020-143106_20382351","dateCreated":"2019-10-20T14:31:06+0000","dateStarted":"2019-10-21T01:10:40+0000","dateFinished":"2019-10-21T01:10:56+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:256"},{"text":"%spark\n\n//Reading, transforming and saving CSV file\nval employeeDF = spark\n      .read\n      .option(\"header\", \"true\")\n      .option(\"inferSchema\", \"true\")\n      .csv(\"/northwind/SourceFullPull/employees.csv\");\n//employeeDF.show();\n//Enabling SQL queries\nemployeeDF.createOrReplaceTempView(\"employees\");\n//Running Simple SQL\nval seattleEmployeeDF = sqlContext.sql(\"SELECT * FROM employees WHERE City='Seattle'\");\n//Transformed Dataframe\nseattleEmployeeDF.show();\n\n//format can be \"csv\", \"json\", \"com.databricks.spark.avro\" , \"parquet\" , \"xml\", \"orc\"\n\nseattleEmployeeDF.write.format(\"csv\").mode(\"overwrite\").save(\"/SampleData/seattleEmployees.csv\")\n\nseattleEmployeeDF.write.format(\"json\").mode(\"overwrite\").save(\"/SampleData/seattleEmployees.json\")\n\nseattleEmployeeDF.write.format(\"avro\").mode(\"overwrite\").save(\"/SampleData/seattleEmployees.avro\")\n\nseattleEmployeeDF.write.format(\"parquet\").mode(\"overwrite\").save(\"/SampleData/seattleEmployees.parquet\")\n\nseattleEmployeeDF.write.format(\"orc\").mode(\"overwrite\").save(\"/SampleData/seattleEmployees.orc\")\n\n// Append\n// Append mode means that when saving a DataFrame to a data source, if data/table already exists, contents of the DataFrame are expected to be appended to existing data.\n// ErrorIfExists\n// ErrorIfExists mode means that when saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown.\n// Ignore\n// Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data.\n// Overwrite\n// Overwrite mode means that when saving a DataFrame to a data source, if data/table already exists, existing data is expected to be overwritten by the contents of the DataFrame.\n","user":"admin","dateUpdated":"2019-10-21T01:10:57+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+--------+---------+--------------------+---------------+-------------------+-------------------+--------------------+-------+------+----------+-------+--------------+---------+--------------------+--------------------+---------+--------------------+\n|EmployeeID|LastName|FirstName|               Title|TitleOfCourtesy|          BirthDate|           HireDate|             Address|   City|Region|PostalCode|Country|     HomePhone|Extension|               Photo|               Notes|ReportsTo|           PhotoPath|\n+----------+--------+---------+--------------------+---------------+-------------------+-------------------+--------------------+-------+------+----------+-------+--------------+---------+--------------------+--------------------+---------+--------------------+\n|         1| Davolio|    Nancy|Sales Representative|            Ms.|1948-12-08 00:00:00|1992-05-01 00:00:00|507 - 20th Ave. E...|Seattle|    WA|     98122|    USA|(206) 555-9857|   5467.0|0x151C2F000200000...|Education include...|        2|http://accweb/emm...|\n|         8|Callahan|    Laura|Inside Sales Coor...|            Ms.|1958-01-09 00:00:00|1994-03-05 00:00:00|4726 - 11th Ave. ...|Seattle|    WA|     98105|    USA|(206) 555-1189|   2344.0|0x151C2F000200000...|Laura received a ...|        2|http://accweb/emm...|\n+----------+--------+---------+--------------------+---------------+-------------------+-------------------+--------------------+-------+------+----------+-------+--------------+---------+--------------------+--------------------+---------+--------------------+\n\nemployeeDF: org.apache.spark.sql.DataFrame = [EmployeeID: int, LastName: string ... 16 more fields]\nseattleEmployeeDF: org.apache.spark.sql.DataFrame = [EmployeeID: int, LastName: string ... 16 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://master:4040/jobs/job?id=0","http://master:4040/jobs/job?id=1","http://master:4040/jobs/job?id=2","http://master:4040/jobs/job?id=3","http://master:4040/jobs/job?id=4","http://master:4040/jobs/job?id=5","http://master:4040/jobs/job?id=6","http://master:4040/jobs/job?id=7"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1571589682088_400103854","id":"20191020-164122_912307141","dateCreated":"2019-10-20T16:41:22+0000","dateStarted":"2019-10-21T01:10:57+0000","dateFinished":"2019-10-21T01:11:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:257"},{"text":"%spark\n//Reading transforming and saving JSON file\nval customerJSON = spark\n      .read\n      .option(\"header\", \"true\")\n      .option(\"inferSchema\", \"true\")\n      .option(\"multiLine\", true)\n      .json(\"/northwind/SourceFullPull/customers.json\");\ncustomerJSON.createOrReplaceTempView(\"customers\");\nval germanCustomers = sqlContext.sql(\"SELECT * FROM customers WHERE Country='Germany'\");\ngermanCustomers.show();\n\ngermanCustomers.write.format(\"json\").mode(\"overwrite\").save(\"/SampleData/germanCustomers.json\")","user":"admin","dateUpdated":"2019-10-21T01:11:26+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------+--------------+--------------------+-------------------+--------------------+-------+----------+------------+------------+----------+------+\n|           Address|          City|         CompanyName|        ContactName|        ContactTitle|Country|CustomerID|         Fax|       Phone|PostalCode|Region|\n+------------------+--------------+--------------------+-------------------+--------------------+-------+----------+------------+------------+----------+------+\n|     Obere Str. 57|        Berlin| Alfreds Futterkiste|       Maria Anders|Sales Representative|Germany|     ALFKI| 030-0076545| 030-0074321|     12209|  NULL|\n|    Forsterstr. 57|      Mannheim|Blauer See Delika...|         Hanna Moos|Sales Representative|Germany|     BLAUS|  0621-08924|  0621-08460|     68306|  NULL|\n|      Walserweg 21|        Aachen|Drachenblut Delik...|       Sven Ottlieb| Order Administrator|Germany|     DRACD| 0241-059428| 0241-039123|     52066|  NULL|\n| Berliner Platz 43|       München|      Frankenversand|      Peter Franken|   Marketing Manager|Germany|     FRANK| 089-0877451| 089-0877310|     80805|  NULL|\n|     Maubelstr. 90|   Brandenburg|     Königlich Essen|      Philip Cramer|     Sales Associate|Germany|     KOENE|        NULL|  0555-09876|     14776|  NULL|\n|      Magazinweg 7|Frankfurt a.M.| Lehmanns Marktstand|     Renate Messner|Sales Representative|Germany|     LEHMS| 069-0245874| 069-0245984|     60528|  NULL|\n|       Heerstr. 22|       Leipzig|Morgenstern Gesun...|    Alexander Feuer| Marketing Assistant|Germany|     MORGK|        NULL| 0342-023176|     04179|  NULL|\n|Mehrheimerstr. 369|          Köln|  Ottilies Käseladen|Henriette Pfalzheim|               Owner|Germany|     OTTIK|0221-0765721|0221-0644327|     50739|  NULL|\n|  Taucherstraße 10|     Cunewalde|          QUICK-Stop|        Horst Kloss|  Accounting Manager|Germany|     QUICK|        NULL| 0372-035188|     01307|  NULL|\n|     Luisenstr. 48|       Münster|  Toms Spezialitäten|      Karin Josephs|   Marketing Manager|Germany|     TOMSP| 0251-035695| 0251-031259|     44087|  NULL|\n| Adenauerallee 900|     Stuttgart|   Die Wandernde Kuh|        Rita Müller|Sales Representative|Germany|     WANDK| 0711-035428| 0711-020361|     70563|  NULL|\n+------------------+--------------+--------------------+-------------------+--------------------+-------+----------+------------+------------+----------+------+\n\ncustomerJSON: org.apache.spark.sql.DataFrame = [Address: string, City: string ... 9 more fields]\ngermanCustomers: org.apache.spark.sql.DataFrame = [Address: string, City: string ... 9 more fields]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://master:4040/jobs/job?id=8","http://master:4040/jobs/job?id=9","http://master:4040/jobs/job?id=10"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1571590424921_717477279","id":"20191020-165344_1701279916","dateCreated":"2019-10-20T16:53:44+0000","dateStarted":"2019-10-21T01:11:26+0000","dateFinished":"2019-10-21T01:11:28+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:258"},{"text":"%spark\n//Reading transforming and saving xml file\nimport com.databricks.spark.xml._\n\nval booksDF = spark.read\n  .option(\"rowTag\", \"book\")\n  .xml(\"/SampleData/books.xml\")\nbooksDF.createOrReplaceTempView(\"books\");\nval books = sqlContext.sql(\"SELECT * FROM books\");\nbooks.show();\n\nval selectedData = booksDF.select(\"author\", \"_id\")\nselectedData.write\n  .option(\"rootTag\", \"books\")\n  .option(\"rowTag\", \"book\")\n  .mode(\"overwrite\")\n  .xml(\"/SampleData/newbooks.xml\")\n  ","user":"admin","dateUpdated":"2019-10-21T01:11:28+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n|  _id|              author|         description|          genre|price|publish_date|               title|\n+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n|bk101|Gambardella, Matthew|\n\n\n         An in...|       Computer|44.95|  2000-10-01|XML Developer's G...|\n|bk102|          Ralls, Kim|A former architec...|        Fantasy| 5.95|  2000-12-16|       Midnight Rain|\n|bk103|         Corets, Eva|After the collaps...|        Fantasy| 5.95|  2000-11-17|     Maeve Ascendant|\n|bk104|         Corets, Eva|In post-apocalyps...|        Fantasy| 5.95|  2001-03-10|     Oberon's Legacy|\n|bk105|         Corets, Eva|The two daughters...|        Fantasy| 5.95|  2001-09-10|  The Sundered Grail|\n|bk106|    Randall, Cynthia|When Carla meets ...|        Romance| 4.95|  2000-09-02|         Lover Birds|\n|bk107|      Thurman, Paula|A deep sea diver ...|        Romance| 4.95|  2000-11-02|       Splish Splash|\n|bk108|       Knorr, Stefan|An anthology of h...|         Horror| 4.95|  2000-12-06|     Creepy Crawlies|\n|bk109|        Kress, Peter|After an inadvert...|Science Fiction| 6.95|  2000-11-02|        Paradox Lost|\n|bk110|        O'Brien, Tim|Microsoft's .NET ...|       Computer|36.95|  2000-12-09|Microsoft .NET: T...|\n|bk111|        O'Brien, Tim|The Microsoft MSX...|       Computer|36.95|  2000-12-01|MSXML3: A Compreh...|\n|bk112|         Galos, Mike|Microsoft Visual ...|       Computer|49.95|  2001-04-16|Visual Studio 7: ...|\n+-----+--------------------+--------------------+---------------+-----+------------+--------------------+\n\nimport com.databricks.spark.xml._\nbooksDF: org.apache.spark.sql.DataFrame = [_id: string, author: string ... 5 more fields]\nbooks: org.apache.spark.sql.DataFrame = [_id: string, author: string ... 5 more fields]\nselectedData: org.apache.spark.sql.DataFrame = [author: string, _id: string]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://master:4040/jobs/job?id=11","http://master:4040/jobs/job?id=12","http://master:4040/jobs/job?id=13"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1571590438640_-1423270232","id":"20191020-165358_418705148","dateCreated":"2019-10-20T16:53:58+0000","dateStarted":"2019-10-21T01:11:28+0000","dateFinished":"2019-10-21T01:11:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:259"},{"text":"%spark\n//get data quality framework\nimport de.frosner.ddq.core._\nimport de.frosner.ddq.constraints._\nimport de.frosner.ddq.reporters.ZeppelinReporter\nval reporter = ZeppelinReporter()\n\n\n//Load some data\nval customerDF = spark\n      .read\n      .option(\"header\", \"true\")\n      .option(\"inferSchema\", \"true\")\n      .csv(\"/northwind/SourceFullPull/customers.csv\");\n\n//See data (debugging)\ncustomerDF.show(2)\n\n//Check quality rules\nCheck(customerDF)\n  .hasNumRows(_ >= 91) //Number of rows rule\n  .hasUniqueKey(\"CustomerID\") //Checking if column has unique values\n  .run()\n  //.run(reporter)\n\n","user":"admin","dateUpdated":"2019-10-21T01:11:31+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{"0":{"graph":{"mode":"table","height":122.667,"optionOpen":false}}},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+--------------------+------------+--------------------+--------------------+-----------+------+----------+-------+------------+------------+\n|CustomerID|         CompanyName| ContactName|        ContactTitle|             Address|       City|Region|PostalCode|Country|       Phone|         Fax|\n+----------+--------------------+------------+--------------------+--------------------+-----------+------+----------+-------+------------+------------+\n|     ALFKI| Alfreds Futterkiste|Maria Anders|Sales Representative|       Obere Str. 57|     Berlin|  NULL|     12209|Germany| 030-0074321| 030-0076545|\n|     ANATR|Ana Trujillo Empa...|Ana Trujillo|               Owner|Avda. de la Const...|México D.F.|  NULL|     05021| Mexico|(5) 555-4729|(5) 555-3745|\n+----------+--------------------+------------+--------------------+--------------------+-----------+------+----------+-------+------------+------------+\nonly showing top 2 rows\n\n\u001b[34mChecking [CustomerID: string, CompanyName: string ... 9 more fields]\u001b[0m\n\u001b[34mIt has a total number of 11 columns and 91 rows.\u001b[0m\n\u001b[32m- The number of rows satisfies (count >= 91).\u001b[0m\n\u001b[32m- Column CustomerID is a key.\u001b[0m\n\nimport de.frosner.ddq.core._\nimport de.frosner.ddq.constraints._\nimport de.frosner.ddq.reporters.ZeppelinReporter\nreporter: de.frosner.ddq.reporters.ZeppelinReporter = ZeppelinReporter(java.io.PrintStream@d97d580)\ncustomerDF: org.apache.spark.sql.DataFrame = [CustomerID: string, CompanyName: string ... 9 more fields]\nres4: de.frosner.ddq.core.CheckResult = CheckResult(Map(NumberOfRowsConstraint((count >= 91)) -> NumberOfRowsConstraintResult(NumberOfRowsConstraint((count >= 91)),91,de.frosner.ddq.constraints.ConstraintSuccess$@3fe7d05), UniqueKeyConstraint(List(CustomerID)) -> UniqueKeyConstraintResult(UniqueKeyConstraint(List(CustomerID)),Some(UniqueKeyConstraintResultData(0)),de.frosner.ddq.constraints.ConstraintSuccess$@3fe7d05)),Check([CustomerID: string, CompanyName: string ... 9 mo..."}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://master:4040/jobs/job?id=14","http://master:4040/jobs/job?id=15","http://master:4040/jobs/job?id=16","http://master:4040/jobs/job?id=17","http://master:4040/jobs/job?id=18","http://master:4040/jobs/job?id=19","http://master:4040/jobs/job?id=20"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1571582734524_-941183629","id":"20191020-144534_1763405067","dateCreated":"2019-10-20T14:45:34+0000","dateStarted":"2019-10-21T01:11:31+0000","dateFinished":"2019-10-21T01:11:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:260"},{"text":"%spark\nimport de.frosner.ddq.core._\nimport de.frosner.ddq.constraints._\nimport de.frosner.ddq.reporters.ZeppelinReporter\nval reporter = ZeppelinReporter()\n\n//More on data quality testing\n//https://github.com/FRosner/drunken-data-quality/wiki/Drunken-Data-Quality-4.1.1#table-constraints\n\ncase class Customer(id: Int, name: String)\ncase class Contract(id: Int, customerId: Int, duration: Int)\n\nval customers = spark.createDataFrame(List(\n  Customer(0, \"Frank\"),\n  Customer(1, \"Alex\"),\n  Customer(2, \"Slavo\")\n))\n\nval contracts = spark.createDataFrame(List(\n  Contract(0, 0, 5),\n  Contract(1, 0, 10),\n  Contract(0, 1, 6)\n))\n\n\nCheck(customers)\n  .hasNumRows(_ >= 3)\n  .hasUniqueKey(\"id\")\n  .run(reporter)\n\nCheck(contracts)\n  .hasNumRows(_ > 0)\n  .hasUniqueKey(\"id\", \"customerId\")\n  .satisfies(\"duration > 0\")\n  .hasForeignKey(customers, \"customerId\" -> \"id\")\n  .run(reporter)\n\nval numRowsConstraint = Check.hasNumRows(_ <= 3)\nval uniqueKeyConstraint = Check.hasUniqueKey(\"id\", \"customerId\")\nval durationConstraint = Check.satisfies(\"duration > 0\")\n\nval check = Check(contracts)\n  .addConstraint(numRowsConstraint)\n  .addConstraint(uniqueKeyConstraint)\n  .addConstraint(durationConstraint)\n\nval results = Runner.run(Seq(check), Seq.empty)\n\nval constraintResults = results(check).constraintResults\n\nprintln(constraintResults(numRowsConstraint).status.stringValue);\n\nassert(constraintResults(numRowsConstraint).status.stringValue == \"Success\")\n\nassert(constraintResults(uniqueKeyConstraint).status.stringValue == \"Success\")\n\n","user":"admin","dateUpdated":"2019-10-21T01:11:40+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"</p>\n<h4>Checking [id: int, name: string]</h4>\n<h5>It has a total number of 2 columns and 3 rows.</h5>\n<table>\n<tr><td style=\"padding:3px\">&#9989;</td><td style=\"padding:3px\">The number of rows satisfies (count >= 3).</td></tr>\n<tr><td style=\"padding:3px\">&#9989;</td><td style=\"padding:3px\">Column id is a key.</td></tr>\n</table>\n<p hidden>\n</p>\n<h4>Checking [id: int, customerId: int ... 1 more field]</h4>\n<h5>It has a total number of 3 columns and 3 rows.</h5>\n<table>\n<tr><td style=\"padding:3px\">&#9989;</td><td style=\"padding:3px\">The number of rows satisfies (count > 0).</td></tr>\n<tr><td style=\"padding:3px\">&#9989;</td><td style=\"padding:3px\">Columns id, customerId are a key.</td></tr>\n<tr><td style=\"padding:3px\">&#9989;</td><td style=\"padding:3px\">Constraint duration > 0 is satisfied.</td></tr>\n<tr><td style=\"padding:3px\">&#9989;</td><td style=\"padding:3px\">Column customerId->id defines a foreign key pointing to the reference table [id: int, name: string].</td></tr>\n</table>\n<p hidden>\nSuccess\nimport de.frosner.ddq.core._\nimport de.frosner.ddq.constraints._\nimport de.frosner.ddq.reporters.ZeppelinReporter\nreporter: de.frosner.ddq.reporters.ZeppelinReporter = ZeppelinReporter(java.io.PrintStream@6cea8434)\ndefined class Customer\ndefined class Contract\ncustomers: org.apache.spark.sql.DataFrame = [id: int, name: string]\ncontracts: org.apache.spark.sql.DataFrame = [id: int, customerId: int ... 1 more field]\nnumRowsConstraint: de.frosner.ddq.constraints.Constraint = NumberOfRowsConstraint((count <= 3))\nuniqueKeyConstraint: de.frosner.ddq.constraints.Constraint = UniqueKeyConstraint(List(id, customerId))\ndurationConstraint: de.frosner.ddq.constraints.Constraint = StringColumnConstraint(duration > 0)\ncheck: de.frosner.ddq.core.Check = Check([id: int, customerId: int ... 1 more field]..."}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://master:4040/jobs/job?id=21","http://master:4040/jobs/job?id=22","http://master:4040/jobs/job?id=23","http://master:4040/jobs/job?id=24","http://master:4040/jobs/job?id=25","http://master:4040/jobs/job?id=26","http://master:4040/jobs/job?id=27","http://master:4040/jobs/job?id=28","http://master:4040/jobs/job?id=29","http://master:4040/jobs/job?id=30","http://master:4040/jobs/job?id=31","http://master:4040/jobs/job?id=32","http://master:4040/jobs/job?id=33","http://master:4040/jobs/job?id=34","http://master:4040/jobs/job?id=35","http://master:4040/jobs/job?id=36","http://master:4040/jobs/job?id=37","http://master:4040/jobs/job?id=38"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1571583395190_299076695","id":"20191020-145635_1137051648","dateCreated":"2019-10-20T14:56:35+0000","dateStarted":"2019-10-21T01:11:40+0000","dateFinished":"2019-10-21T01:12:05+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:261"},{"text":"import io.delta.tables._\nimport org.apache.spark.sql.functions._\n//Delta lake implementation https://delta.io/, https://docs.azuredatabricks.net/delta/index.html\n\n//Creating a delta lake table\nval dataCreate = spark.range(0, 5)\ndataCreate.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\n\n//Reading from a delta lake table\nval dataRead = spark.read.format(\"delta\").load(\"/tmp/delta-table\")\ndataRead.show()\n\n//Overwrite a delta lake table\nval dataOverWrite = spark.range(5, 10)\ndataOverWrite.write.format(\"delta\").mode(\"overwrite\").save(\"/tmp/delta-table\")\ndataOverWrite.show()\n\n\n//Update without overwrite\n\nval deltaTable = DeltaTable.forPath(\"/tmp/delta-table\")\n\n// Update every even value by adding 100 to it\ndeltaTable.update(\n  condition = expr(\"id % 2 == 0\"),\n  set = Map(\"id\" -> expr(\"id + 100\")))\n\n// Delete every even value\ndeltaTable.delete(condition = expr(\"id % 2 == 0\"))\n\n// Upsert (merge) new data\nval newData = spark.range(0, 20).toDF\n\ndeltaTable.as(\"oldData\")\n  .merge(\n    newData.as(\"newData\"),\n    \"oldData.id = newData.id\")\n  .whenMatched\n  .update(Map(\"id\" -> col(\"newData.id\")))\n  .whenNotMatched\n  .insert(Map(\"id\" -> col(\"newData.id\")))\n  .execute()\n\ndeltaTable.toDF.show()\n\n\n// //Read older version of data using version\n\nval dataVersion = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(\"/tmp/delta-table\")\ndataVersion.show()\n","user":"admin","dateUpdated":"2019-10-21T01:12:05+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+\n| id|\n+---+\n|  2|\n|  3|\n|  4|\n|  0|\n|  1|\n+---+\n\n+---+\n| id|\n+---+\n|  5|\n|  6|\n|  7|\n|  8|\n|  9|\n+---+\n\n+---+\n| id|\n+---+\n| 19|\n|  5|\n| 11|\n| 14|\n|  9|\n| 16|\n|  0|\n|  4|\n| 17|\n|  8|\n| 13|\n|  1|\n|  6|\n|  7|\n| 18|\n| 15|\n| 12|\n|  3|\n|  2|\n| 10|\n+---+\n\n+---+\n| id|\n+---+\n|  2|\n|  3|\n|  4|\n|  0|\n|  1|\n+---+\n\nimport io.delta.tables._\nimport org.apache.spark.sql.functions._\ndataCreate: org.apache.spark.sql.Dataset[Long] = [id: bigint]\ndataRead: org.apache.spark.sql.DataFrame = [id: bigint]\ndataOverWrite: org.apache.spark.sql.Dataset[Long] = [id: bigint]\ndeltaTable: io.delta.tables.DeltaTable = io.delta.tables.DeltaTable@2211dcc6\nnewData: org.apache.spark.sql.DataFrame = [id: bigint]\ndataVersion: org.apache.spark.sql.DataFrame = [id: bigint]\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://master:4040/jobs/job?id=39","http://master:4040/jobs/job?id=40","http://master:4040/jobs/job?id=41","http://master:4040/jobs/job?id=42","http://master:4040/jobs/job?id=43","http://master:4040/jobs/job?id=44","http://master:4040/jobs/job?id=45","http://master:4040/jobs/job?id=46","http://master:4040/jobs/job?id=47","http://master:4040/jobs/job?id=48","http://master:4040/jobs/job?id=49","http://master:4040/jobs/job?id=50","http://master:4040/jobs/job?id=51","http://master:4040/jobs/job?id=52","http://master:4040/jobs/job?id=53","http://master:4040/jobs/job?id=54","http://master:4040/jobs/job?id=55","http://master:4040/jobs/job?id=56","http://master:4040/jobs/job?id=57","http://master:4040/jobs/job?id=58","http://master:4040/jobs/job?id=59","http://master:4040/jobs/job?id=60","http://master:4040/jobs/job?id=61","http://master:4040/jobs/job?id=62","http://master:4040/jobs/job?id=63","http://master:4040/jobs/job?id=64","http://master:4040/jobs/job?id=65","http://master:4040/jobs/job?id=66","http://master:4040/jobs/job?id=67","http://master:4040/jobs/job?id=68","http://master:4040/jobs/job?id=69","http://master:4040/jobs/job?id=70","http://master:4040/jobs/job?id=71","http://master:4040/jobs/job?id=72","http://master:4040/jobs/job?id=73"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1571589247089_-466941740","id":"20191020-163407_719574535","dateCreated":"2019-10-20T16:34:07+0000","dateStarted":"2019-10-21T01:12:05+0000","dateFinished":"2019-10-21T01:14:10+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:262"},{"user":"admin","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1571594194660_1337186527","id":"20191020-175634_647266888","dateCreated":"2019-10-20T17:56:34+0000","status":"RUNNING","progressUpdateIntervalMs":500,"$$hashKey":"object:263","text":"%spark\nimport io.delta.tables._\nimport org.apache.spark.sql.functions._\n\n//Initial load full table\nval customerFullData = spark\n      .read\n      .option(\"header\", \"true\")\n      .option(\"inferSchema\", \"true\")\n      .option(\"multiLine\", true)\n      .json(\"/northwind/SourceFullPull/customers.json\");\n\n//Saved it as delta table\ncustomerFullData.write.format(\"delta\").mode(\"overwrite\").partitionBy(\"Country\").save(\"/tmp/customerdata\")\n\n\n\n//Loading delta customers from another csv file\nval customerdeltaData = spark\n      .read\n      .option(\"header\", \"true\")\n      .option(\"inferSchema\", \"true\")\n      .csv(\"/northwind/SourceDeltaUpdate/customers.csv\");\n\n//customerdeltaData.write.format(\"delta\").partitionBy(\"Country\")..mode(\"overwrite\").save(\"/tmp/customerdeltadata\")\n\n//Now loaded full table to perform merge\nval fullCustomerTable = DeltaTable.forPath(\"/tmp/customerdata\")\n\n// Upsert (merge) new data\n\nfullCustomerTable.as(\"oldData\")\n  .merge(\n    customerdeltaData.as(\"newData\"),\n    \"oldData.CustomerID = newData.CustomerID\")\n  .whenMatched\n  .update(Map(\n      \"CustomerID\" -> col(\"newData.CustomerID\")\n      ,\"CompanyName\" -> col(\"newData.CompanyName\")\n      ,\"ContactName\" -> col(\"newData.ContactName\")\n      ,\"ContactTitle\" -> col(\"newData.ContactTitle\")\n      ,\"Address\" -> col(\"newData.Address\")\n      ,\"City\" -> col(\"newData.City\")\n      ,\"Region\" -> col(\"newData.Region\")\n      ,\"PostalCode\" -> col(\"newData.PostalCode\")\n      ,\"Country\" -> col(\"newData.Country\")\n      ,\"Phone\" -> col(\"newData.Phone\")\n      ,\"Fax\" -> col(\"newData.Fax\")\n      ))\n  .whenNotMatched\n  .insert(Map(\n      \"CustomerID\" -> col(\"newData.CustomerID\")\n      ,\"CompanyName\" -> col(\"newData.CompanyName\")\n      ,\"ContactName\" -> col(\"newData.ContactName\")\n      ,\"ContactTitle\" -> col(\"newData.ContactTitle\")\n      ,\"Address\" -> col(\"newData.Address\")\n      ,\"City\" -> col(\"newData.City\")\n      ,\"Region\" -> col(\"newData.Region\")\n      ,\"PostalCode\" -> col(\"newData.PostalCode\")\n      ,\"Country\" -> col(\"newData.Country\")\n      ,\"Phone\" -> col(\"newData.Phone\")\n      ,\"Fax\" -> col(\"newData.Fax\")\n      ))\n  .execute()\n\nfullCustomerTable.toDF.show()\n\n","dateUpdated":"2019-10-21T01:14:10+0000","dateFinished":"2019-10-21T01:09:24+0000","dateStarted":"2019-10-21T01:14:10+0000","errorMessage":"","runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://master:4040/jobs/job?id=74","http://master:4040/jobs/job?id=75"],"interpreterSettingId":"spark"}}},{"text":"%sql\n","user":"admin","dateUpdated":"2019-10-21T00:15:02+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1571616902207_1950554718","id":"20191021-001502_1007374165","dateCreated":"2019-10-21T00:15:02+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1077"}],"name":"ETLBasics","id":"2EQUD5ZGD","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}